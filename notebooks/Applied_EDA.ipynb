{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbdc113",
   "metadata": {},
   "source": [
    "# Task-1 EDA  and  Data Preprocessing \n",
    "Major activities accomplished under this task:\n",
    "\n",
    " - The full CFPB complaint dataset was loaded\n",
    " - initial EDA to understand the data.\n",
    " - Filtering of dataset to meet the project requirements \n",
    " - cleaning of text narrative to improve the embedding quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa45783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# installed dependencies \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6002183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Processing chunk 73...\n",
      "Processing chunk 74...\n",
      "Processing chunk 75...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize counters\n",
    "total_raw = 0\n",
    "total_with_narrative = 0\n",
    "total_without_narrative = 0\n",
    "\n",
    "# Initialize an empty DataFrame to hold all data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Process the CSV in chunks\n",
    "for i, chunk in enumerate(pd.read_csv('F:/Intelligent_Complaint_Analysis/data/Complaints.csv',\n",
    "                                      chunksize=100000, low_memory=False)):\n",
    "\n",
    "    print(f'Processing chunk {i+1}...')\n",
    "\n",
    "    # Clean column names\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "    # Update total row count\n",
    "    total_raw += len(chunk)\n",
    "\n",
    "    # Select and rename relevant columns\n",
    "    chunk = chunk[['Complaint ID', 'Product', 'Consumer complaint narrative']].copy()\n",
    "    chunk.columns = ['complaint_id', 'product', 'narrative']\n",
    "\n",
    "    # Update narrative counts\n",
    "    total_with_narrative += chunk['narrative'].notna().sum()\n",
    "    total_without_narrative += chunk['narrative'].isna().sum()\n",
    "\n",
    "    # Append the chunk to the final DataFrame\n",
    "    if i == 0:\n",
    "        df = chunk\n",
    "    else:\n",
    "        df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nâœ… Total rows processed: {total_raw}\")\n",
    "print(f\"ðŸŸ¢ Rows with narratives: {total_with_narrative}\")\n",
    "print(f\"ðŸ”´ Rows without narratives: {total_without_narrative}\")\n",
    "print(f\"ðŸ“„ Final DataFrame shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4f8e3",
   "metadata": {},
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eca120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.EDA_preprocessing import load_and_process_complaints\n",
    "file_path = 'F:/Intelligent_Complaint_Analysis/data/Complaints.csv'\n",
    "df_complaints, summary = load_and_process_complaints(file_path)\n",
    "\n",
    "print(\"\\nðŸ“Š Processing Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df_complaints.to_csv(\"F:/Intelligent_Complaint_Analysis/data/cleaned_complaints.csv\", index=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial EDA\n",
    "df=df_complaints\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feac3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929f55ca",
   "metadata": {},
   "source": [
    "## Distribution of complaints across Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54eee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of complaints across Products\n",
    "print(\"\\nComplaint Distribution by Product:\")\n",
    "product_counts = df['Product'].value_counts()\n",
    "print(product_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71580f12",
   "metadata": {},
   "source": [
    "## visualization of Complaint by product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f924a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize product distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=product_counts.values, y=product_counts.index)\n",
    "plt.title('Distribution of Complaints by Product')\n",
    "plt.xlabel('Number of Complaints')\n",
    "plt.ylabel('Product')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189acb5",
   "metadata": {},
   "source": [
    "## Narrative Length Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301eec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate narrative length (word count)\n",
    "df['narrative_length'] = df['Consumer complaint narrative'].apply(\n",
    "    lambda x: len(word_tokenize(str(x))) if pd.notnull(x) else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize narrative length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['narrative_length'], bins=50)\n",
    "plt.title('Distribution of Consumer Complaint Narrative Lengths')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba36023",
   "metadata": {},
   "source": [
    "**Complaints with and without Narrative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify complaints with and without narratives\n",
    "narrative_counts = df['Consumer complaint narrative'].notnull().value_counts()\n",
    "print(\"\\nComplaints with and without narratives:\")\n",
    "print(f\"With narratives: {narrative_counts[True]}\")\n",
    "print(f\"Without narratives: {narrative_counts[False]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d09cd7",
   "metadata": {},
   "source": [
    "## Data Filtering and Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset for specified products and non-empty narratives\n",
    "target_products = ['Credit card', 'Personal loan', 'Buy Now, Pay Later (BNPL)', \n",
    "                  'Savings account', 'Money transfers']\n",
    "df_filtered = df[\n",
    "    (df['Product'].isin(target_products)) & \n",
    "    (df['Consumer complaint narrative'].notnull())\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "from src.EDA_preprocessing import clean_narrative\n",
    "df_filtered['cleaned_narrative'] = df_filtered['Consumer complaint narrative'].apply(clean_narrative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521fa1b",
   "metadata": {},
   "source": [
    "## Filtered Data was saved to folder data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268abea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned and filtered dataset\n",
    "df_filtered.to_csv('F:/Intelligent_Complaint_Analysis/data/filtered_data.csv', index=False)\n",
    "print(f\"\\nFiltered dataset saved to data\")\n",
    "print(f\"Shape of filtered dataset: {df_filtered.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adc7b3",
   "metadata": {},
   "source": [
    "## Summary statistics of filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics of filtered dataset\n",
    "print(\"\\nFiltered Dataset Statistics:\")\n",
    "print(df_filtered.describe(include='all'))\n",
    "print(\"\\nProduct Distribution in Filtered Dataset:\")\n",
    "print(df_filtered['Product'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
